"""
Automated QA System

Automatically tests AI-generated code before deployment.

Features:
- Auto-generate tests from tickets
- Run tests on PR creation
- Deploy to QA environment
- E2E testing
- Test coverage tracking
- Comment results on PR
"""

import logging
import os
import subprocess
from typing import Dict, List, Optional
from datetime import datetime
import asyncio

from learning.deepseek_coder import DeepSeekCoder

logger = logging.getLogger(__name__)


class AutomatedQASystem:
    """
    Automated testing and QA deployment
    
    Workflow:
    1. PR created with AI-generated code
    2. Run unit tests
    3. Deploy to QA environment
    4. Run E2E tests
    5. Generate coverage report
    6. Comment results on PR
    7. Approve or request changes
    """
    
    def __init__(self):
        self.deepseek = DeepSeekCoder(model_size="6.7b")
        
    async def process_pr(
        self,
        pr_id: str,
        repo_path: str,
        project_config: Dict
    ) -> Dict:
        """
        Process PR through automated QA pipeline
        
        Args:
            pr_id: Pull request ID
            repo_path: Local repo path
            project_config: Project configuration
            
        Returns:
            {
                'success': bool,
                'tests_passed': bool,
                'coverage': float,
                'e2e_passed': bool,
                'qa_url': str,
                'report': str
            }
        """
        
        logger.info(f"ðŸ§ª Processing PR #{pr_id} through QA pipeline")
        
        results = {
            'pr_id': pr_id,
            'started_at': datetime.now().isoformat(),
            'success': False
        }
        
        try:
            # 1. Checkout PR branch
            await self._checkout_pr(pr_id, repo_path)
            
            # 2. Install dependencies
            await self._install_dependencies(repo_path, project_config)
            
            # 3. Run unit tests
            unit_results = await self._run_unit_tests(repo_path, project_config)
            results['unit_tests'] = unit_results
            
            if not unit_results['passed']:
                results['report'] = "Unit tests failed"
                await self._comment_on_pr(pr_id, results)
                return results
                
            # 4. Generate coverage report
            coverage = await self._generate_coverage(repo_path, project_config)
            results['coverage'] = coverage
            
            # 5. Deploy to QA environment
            qa_deployment = await self._deploy_to_qa(pr_id, repo_path, project_config)
            results['qa_deployment'] = qa_deployment
            
            if not qa_deployment['success']:
                results['report'] = "QA deployment failed"
                await self._comment_on_pr(pr_id, results)
                return results
                
            # 6. Run E2E tests
            e2e_results = await self._run_e2e_tests(
                qa_deployment['url'],
                repo_path,
                project_config
            )
            results['e2e_tests'] = e2e_results
            
            # 7. Generate final report
            results['success'] = (
                unit_results['passed'] and
                e2e_results['passed'] and
                coverage['percentage'] >= project_config.get('min_coverage', 80)
            )
            
            results['report'] = self._generate_report(results)
            
            # 8. Comment on PR
            await self._comment_on_pr(pr_id, results)
            
            logger.info(f"âœ… QA pipeline complete for PR #{pr_id}")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ QA pipeline failed: {e}")
            results['error'] = str(e)
            results['report'] = f"Pipeline error: {e}"
            await self._comment_on_pr(pr_id, results)
            return results
            
    async def _checkout_pr(self, pr_id: str, repo_path: str):
        """Checkout PR branch"""
        logger.info(f"Checking out PR #{pr_id}")
        
        subprocess.run(
            ['gh', 'pr', 'checkout', pr_id],
            cwd=repo_path,
            check=True
        )
        
    async def _install_dependencies(self, repo_path: str, config: Dict):
        """Install project dependencies"""
        logger.info("Installing dependencies")
        
        project_type = config.get('type', 'node')
        
        if project_type == 'node':
            subprocess.run(['npm', 'install'], cwd=repo_path, check=True)
        elif project_type == 'python':
            subprocess.run(['pip', 'install', '-r', 'requirements.txt'], cwd=repo_path, check=True)
        elif project_type == 'rust':
            subprocess.run(['cargo', 'build'], cwd=repo_path, check=True)
        elif project_type == 'go':
            subprocess.run(['go', 'mod', 'download'], cwd=repo_path, check=True)
            
    async def _run_unit_tests(self, repo_path: str, config: Dict) -> Dict:
        """Run unit tests"""
        logger.info("Running unit tests")
        
        project_type = config.get('type', 'node')
        test_command = config.get('test_command')
        
        if not test_command:
            # Default test commands
            if project_type == 'node':
                test_command = 'npm test'
            elif project_type == 'python':
                test_command = 'pytest'
            elif project_type == 'rust':
                test_command = 'cargo test'
            elif project_type == 'go':
                test_command = 'go test ./...'
                
        try:
            result = subprocess.run(
                test_command.split(),
                cwd=repo_path,
                capture_output=True,
                text=True,
                timeout=300  # 5 minutes
            )
            
            return {
                'passed': result.returncode == 0,
                'output': result.stdout,
                'errors': result.stderr,
                'duration': 0  # TODO: Parse from output
            }
            
        except subprocess.TimeoutExpired:
            return {
                'passed': False,
                'output': '',
                'errors': 'Tests timed out after 5 minutes',
                'duration': 300
            }
            
    async def _generate_coverage(self, repo_path: str, config: Dict) -> Dict:
        """Generate test coverage report"""
        logger.info("Generating coverage report")
        
        project_type = config.get('type', 'node')
        
        try:
            if project_type == 'node':
                result = subprocess.run(
                    ['npm', 'run', 'coverage'],
                    cwd=repo_path,
                    capture_output=True,
                    text=True,
                    timeout=300
                )
            elif project_type == 'python':
                result = subprocess.run(
                    ['pytest', '--cov', '--cov-report=json'],
                    cwd=repo_path,
                    capture_output=True,
                    text=True,
                    timeout=300
                )
                
            # Parse coverage percentage
            # TODO: Parse from coverage report
            coverage_pct = 85.0  # Placeholder
            
            return {
                'percentage': coverage_pct,
                'report': result.stdout
            }
            
        except Exception as e:
            logger.error(f"Coverage generation failed: {e}")
            return {
                'percentage': 0,
                'report': str(e)
            }
            
    async def _deploy_to_qa(
        self,
        pr_id: str,
        repo_path: str,
        config: Dict
    ) -> Dict:
        """Deploy to QA environment"""
        logger.info(f"Deploying PR #{pr_id} to QA")
        
        # Generate QA URL
        qa_url = f"https://qa-pr-{pr_id}.{config.get('domain', 'example.com')}"
        
        try:
            # Deploy using project-specific method
            deploy_method = config.get('qa_deploy_method', 'docker')
            
            if deploy_method == 'docker':
                # Build Docker image
                subprocess.run(
                    ['docker', 'build', '-t', f'qa-pr-{pr_id}', '.'],
                    cwd=repo_path,
                    check=True
                )
                
                # Run container
                subprocess.run(
                    [
                        'docker', 'run', '-d',
                        '--name', f'qa-pr-{pr_id}',
                        '-p', f'{8000 + int(pr_id)}:8000',
                        f'qa-pr-{pr_id}'
                    ],
                    check=True
                )
                
            elif deploy_method == 'kubernetes':
                # Deploy to k8s
                subprocess.run(
                    ['kubectl', 'apply', '-f', f'k8s/qa-pr-{pr_id}.yaml'],
                    cwd=repo_path,
                    check=True
                )
                
            elif deploy_method == 'juju':
                # Deploy using Juju
                subprocess.run(
                    ['juju', 'deploy', f'./charm', '--channel=edge', f'qa-pr-{pr_id}'],
                    cwd=repo_path,
                    check=True
                )
                
            # Wait for deployment to be ready
            await asyncio.sleep(30)
            
            return {
                'success': True,
                'url': qa_url,
                'method': deploy_method
            }
            
        except Exception as e:
            logger.error(f"QA deployment failed: {e}")
            return {
                'success': False,
                'url': None,
                'error': str(e)
            }
            
    async def _run_e2e_tests(
        self,
        qa_url: str,
        repo_path: str,
        config: Dict
    ) -> Dict:
        """Run E2E tests against QA environment"""
        logger.info(f"Running E2E tests against {qa_url}")
        
        e2e_framework = config.get('e2e_framework', 'playwright')
        
        try:
            if e2e_framework == 'playwright':
                result = subprocess.run(
                    ['npx', 'playwright', 'test', '--base-url', qa_url],
                    cwd=repo_path,
                    capture_output=True,
                    text=True,
                    timeout=600  # 10 minutes
                )
            elif e2e_framework == 'cypress':
                result = subprocess.run(
                    ['npx', 'cypress', 'run', '--config', f'baseUrl={qa_url}'],
                    cwd=repo_path,
                    capture_output=True,
                    text=True,
                    timeout=600
                )
            elif e2e_framework == 'selenium':
                result = subprocess.run(
                    ['python', '-m', 'pytest', 'e2e/', f'--base-url={qa_url}'],
                    cwd=repo_path,
                    capture_output=True,
                    text=True,
                    timeout=600
                )
                
            return {
                'passed': result.returncode == 0,
                'output': result.stdout,
                'errors': result.stderr,
                'framework': e2e_framework
            }
            
        except subprocess.TimeoutExpired:
            return {
                'passed': False,
                'output': '',
                'errors': 'E2E tests timed out after 10 minutes',
                'framework': e2e_framework
            }
        except Exception as e:
            logger.error(f"E2E tests failed: {e}")
            return {
                'passed': False,
                'output': '',
                'errors': str(e),
                'framework': e2e_framework
            }
            
    def _generate_report(self, results: Dict) -> str:
        """Generate QA report"""
        
        report = f"""# QA Report for PR #{results['pr_id']}

## Summary
- **Status:** {'âœ… PASSED' if results['success'] else 'âŒ FAILED'}
- **Started:** {results['started_at']}

## Unit Tests
- **Result:** {'âœ… Passed' if results['unit_tests']['passed'] else 'âŒ Failed'}
- **Duration:** {results['unit_tests']['duration']}s

## Test Coverage
- **Coverage:** {results['coverage']['percentage']:.1f}%
- **Target:** 80%

## QA Deployment
- **URL:** {results['qa_deployment'].get('url', 'N/A')}
- **Method:** {results['qa_deployment'].get('method', 'N/A')}

## E2E Tests
- **Result:** {'âœ… Passed' if results['e2e_tests']['passed'] else 'âŒ Failed'}
- **Framework:** {results['e2e_tests']['framework']}

## Next Steps
"""
        
        if results['success']:
            report += """
âœ… All checks passed! This PR is ready for staging deployment.

To deploy to staging, approve this PR and it will automatically deploy.
"""
        else:
            report += """
âŒ Some checks failed. Please review the errors above and make necessary fixes.

Once fixed, push new commits and the QA pipeline will run again.
"""
        
        return report
        
    async def _comment_on_pr(self, pr_id: str, results: Dict):
        """Comment QA results on PR"""
        logger.info(f"Commenting on PR #{pr_id}")
        
        try:
            subprocess.run(
                ['gh', 'pr', 'comment', pr_id, '--body', results['report']],
                check=True
            )
        except Exception as e:
            logger.error(f"Failed to comment on PR: {e}")


# Global instance
_automated_qa = AutomatedQASystem()


def get_automated_qa() -> AutomatedQASystem:
    """Get global automated QA instance"""
    return _automated_qa
